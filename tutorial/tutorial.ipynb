{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJc7GRtFbrRK"
   },
   "source": [
    "# **Deep Imbalanced Regression (DIR) Tutorial**\n",
    "\n",
    "In this notebook, we provide a hands-on tutorial for DIR on a small-scale dataset, [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), as a quick overview on how to perform practical (deep) imbalanced regression on custom datasets. Since the dataset is small, the training will be very fast and could be done on CPU. We show only the application of Label Distribution Smoothing (LDS); for FDS implementation, please refer to the main repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7mBaSzabN6I"
   },
   "source": [
    "# Download the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWuPbiEYmzvS",
    "outputId": "844cb221-3d2f-44d7-9918-a1f521a9b49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
      "Data already downloaded.\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown\n",
    "\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "if not (os.path.exists('./housing.data.train') and os.path.exists('./housing.data.val') and os.path.exists('./housing.data.test')):\n",
    "    gdown.download(\"https://drive.google.com/uc?export=download&id=14KgStJ_CeQ-AhCMtw_KKAEbK0_FwhSrE\", output='./housing.data.train', quiet=False)\n",
    "    gdown.download(\"https://drive.google.com/uc?export=download&id=1Q8Xu3FuX_pV1tXIlvExFuoixllDELvzv\", output='./housing.data.val', quiet=False)\n",
    "    gdown.download(\"https://drive.google.com/uc?export=download&id=1-w8LDDAOO9uBfevIZhMZNU4TPVSXvRTq\", output='./housing.data.test', quiet=False)\n",
    "else:\n",
    "    print('Data already downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnWsY4FzYC0Y"
   },
   "source": [
    "# Define the deep learning (neural network) model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKm3tqsMhDOA"
   },
   "outputs": [],
   "source": [
    "# fcnet.py: Define the deep learning (neural network) model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, dropout=None):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc0 = nn.Linear(13, layers[0])\n",
    "        self.fc1 = nn.Linear(layers[0], layers[1])\n",
    "        self.fc2 = nn.Linear(layers[1], layers[2])\n",
    "        self.fc_final = nn.Linear(layers[-1], 1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_dropout = True if dropout else False\n",
    "        if self.use_dropout:\n",
    "            print(f'Using dropout: {dropout}')\n",
    "            self.dropout0 = nn.Dropout(p=dropout)\n",
    "            self.dropout1 = nn.Dropout(p=dropout)\n",
    "            self.dropout2 = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout0 = nn.Identity()\n",
    "            self.dropout1 = nn.Identity()\n",
    "            self.dropout2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x, targets=None, epoch=None):\n",
    "        x = self.dropout0(F.relu(self.fc0(x)))\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = self.fc_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def fcnet1(**kwargs):\n",
    "    return FCNet([256, 256, 256], **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWohJXZPYbY3"
   },
   "source": [
    "# Define the loss functions. Here we only need the L1 loss: |f(x) - y|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Kzc8PTjhDIH"
   },
   "outputs": [],
   "source": [
    "# loss.py: Define the loss functions (here we only need the L1 loss)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def weighted_mse_loss(inputs, targets, weights=None):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_l1_loss(inputs, targets, weights=None):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_mse_loss(inputs, targets, activate='sigmoid', beta=.2, gamma=1, weights=None):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_focal_l1_loss(inputs, targets, activate='sigmoid', beta=.2, gamma=1, weights=None):\n",
    "    loss = F.l1_loss(inputs, targets, reduction='none')\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def weighted_huber_loss(inputs, targets, beta=1., weights=None):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    if weights is not None:\n",
    "        loss *= weights.expand_as(loss)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aunbes6NY_Pz"
   },
   "source": [
    "# Define some utility functions (not the focus of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3XXH8Yik4M9"
   },
   "outputs": [],
   "source": [
    "# utils.py: Define some utility functions (not the focus of this course).\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_fmtstr(num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def query_yes_no(question):\n",
    "    \"\"\" Ask a yes/no question via input() and return their answer. \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\n",
    "    prompt = \" [Y/n] \"\n",
    "\n",
    "    while True:\n",
    "        print(question + prompt, end=':')\n",
    "        choice = input().lower()\n",
    "        if choice == '':\n",
    "            return valid['y']\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n",
    "\n",
    "def prepare_folders(args):\n",
    "    folders_util = [args.store_root, os.path.join(args.store_root, args.store_name)]\n",
    "    if os.path.exists(folders_util[-1]) and not args.resume and not args.evaluate:\n",
    "        if query_yes_no('overwrite previous folder: {} ?'.format(folders_util[-1])):\n",
    "            shutil.rmtree(folders_util[-1])\n",
    "            print(folders_util[-1] + ' removed.')\n",
    "        else:\n",
    "            raise RuntimeError('Output folder {} already exists'.format(folders_util[-1]))\n",
    "    for folder in folders_util:\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"===> Creating folder: {folder}\")\n",
    "            os.mkdir(folder)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    lr = args.lr\n",
    "    for milestone in args.schedule:\n",
    "        lr *= 0.1 if epoch >= milestone else 1.\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def save_checkpoint(args, state, is_best, prefix=''):\n",
    "    filename = f\"{args.store_root}/{args.store_name}/{prefix}ckpt.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print(\"===> Saving current best checkpoint...\")\n",
    "        shutil.copyfile(filename, filename.replace('pth.tar', 'best.pth.tar'))\n",
    "\n",
    "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
    "    if torch.sum(v1) < 1e-10:\n",
    "        return matrix\n",
    "    if (v1 == 0.).any():\n",
    "        valid = (v1 != 0.)\n",
    "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
    "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
    "        return matrix\n",
    "\n",
    "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
    "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpbjtnotZUkk"
   },
   "source": [
    "# Define the data iterator (data loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqK8H1UghC_v"
   },
   "outputs": [],
   "source": [
    "# datasets.py: Define the data iterator (data loader).\n",
    "from scipy.ndimage import convolve1d\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "# from utils import get_lds_kernel_window\n",
    "\n",
    "class BostonHousing(data.Dataset):\n",
    "    def __init__(self, data_dir, split='train', reweight='none',\n",
    "                 lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        self.split = split\n",
    "        self.data = np.loadtxt(data_dir, dtype='float32')\n",
    "        self.weights = self._prepare_weights(reweight=reweight, lds=lds, lds_kernel=lds_kernel, lds_ks=lds_ks, lds_sigma=lds_sigma)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index % self.data.shape[0]\n",
    "        feature = self.data[index, :-1]\n",
    "        label = np.expand_dims(np.asarray(self.data[index, -1]), axis=0)\n",
    "        weight = np.asarray([self.weights[index]]).astype('float32') if self.weights is not None else np.asarray([np.float32(1.)])\n",
    "        return feature, label, weight\n",
    "\n",
    "    def _prepare_weights(self, reweight, max_target=51, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2):\n",
    "        assert reweight in {'none', 'inverse', 'sqrt_inv'}\n",
    "        assert reweight != 'none' if lds else True, \\\n",
    "            \"Set reweight to \\'sqrt_inv\\' (default) or \\'inverse\\' when using LDS\"\n",
    "\n",
    "        value_dict = {x: 0 for x in range(max_target)}\n",
    "        labels = self.data[:, -1].tolist()\n",
    "        # mbr\n",
    "        for label in labels:\n",
    "            value_dict[min(max_target - 1, int(label))] += 1\n",
    "        if reweight == 'sqrt_inv':\n",
    "            value_dict = {k: np.sqrt(v) for k, v in value_dict.items()}\n",
    "        elif reweight == 'inverse':\n",
    "            value_dict = {k: np.clip(v, 5, 1000) for k, v in value_dict.items()}  # clip weights for inverse re-weight\n",
    "        num_per_label = [value_dict[min(max_target - 1, int(label))] for label in labels]\n",
    "        if not len(num_per_label) or reweight == 'none':\n",
    "            return None\n",
    "        print(f\"Using re-weighting: [{reweight.upper()}]\")\n",
    "\n",
    "        if lds:\n",
    "            lds_kernel_window = get_lds_kernel_window(lds_kernel, lds_ks, lds_sigma)\n",
    "            print(f'Using LDS: [{lds_kernel.upper()}] ({lds_ks}/{lds_sigma})')\n",
    "            smoothed_value = convolve1d(\n",
    "                np.asarray([v for _, v in value_dict.items()]), weights=lds_kernel_window, mode='constant')\n",
    "            num_per_label = [smoothed_value[min(max_target - 1, int(label))] for label in labels]\n",
    "\n",
    "        weights = [np.float32(1 / x) for x in num_per_label]\n",
    "        scaling = len(weights) / np.sum(weights)\n",
    "        weights = [scaling * x for x in weights]\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcuwqp5paPk3"
   },
   "source": [
    "# Set up some default configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7mtQacPhBoW"
   },
   "outputs": [],
   "source": [
    "# train.py, Part 1: Set up some default configurations.\n",
    "import time\n",
    "import argparse\n",
    "#import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from resnet import resnet50\n",
    "# from fcnet import fcnet1\n",
    "# from loss import *\n",
    "# from datasets import BostonHousing\n",
    "# from utils import *\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\"\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# CPU only\n",
    "parser.add_argument('--cpu_only', action='store_true', default=False, help='whether to use CPU only')\n",
    "# imbalanced related\n",
    "# LDS\n",
    "parser.add_argument('--lds', action='store_true', default=False, help='whether to enable LDS')\n",
    "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
    "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
    "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
    "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
    "\n",
    "# re-weighting: SQRT_INV / INV\n",
    "parser.add_argument('--reweight', type=str, default='none', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
    "\n",
    "# training/optimization related\n",
    "parser.add_argument('--dataset', type=str, default='bostonhousing', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
    "parser.add_argument('--data_dir', type=str, default='./housing.data', help='data directory')\n",
    "parser.add_argument('--model', type=str, default='fcnet1', help='model name')\n",
    "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
    "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
    "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "parser.add_argument('--epoch', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
    "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
    "#parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
    "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
    "parser.add_argument('--workers', type=int, default=32, help='number of workers used in data loading')\n",
    "# checkpoints\n",
    "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
    "\n",
    "parser.set_defaults(augment=True)\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWvzFoYJnwuF"
   },
   "outputs": [],
   "source": [
    "args.cpu_only = True # Use CPU to train/test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtG61vP8ROkP"
   },
   "source": [
    "# Train 4 different models using the 4 options below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wv3tQjBp0dG"
   },
   "outputs": [],
   "source": [
    "# Option 1: To train the basic model, use the default setting, don't need to do anything\n",
    "args.reweight = 'none'\n",
    "args.lds = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYS6r4nyp36_"
   },
   "outputs": [],
   "source": [
    "# Option 2: To train the inverse weighting model:\n",
    "args.reweight = 'inverse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTYBWlx4p6R3"
   },
   "outputs": [],
   "source": [
    "# Option 3: To train the sqrt_inverse weighting model:\n",
    "args.reweight = 'sqrt_inv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2j6V3UEqSoP"
   },
   "outputs": [],
   "source": [
    "# Option 4: To train the Label Distribution Smoothing (LDS) model:\n",
    "args.reweight = 'sqrt_inv'\n",
    "args.lds = True\n",
    "args.lds_kernel = 'gaussian'\n",
    "args.lds_ks = 5 # 5\n",
    "args.lds_sigma = 2 # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nBTkrzbRbjT"
   },
   "source": [
    "# If you do not want to re-train models, download some trained models and directly use the models to predict labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Hbe77zCNJAr",
    "outputId": "78098641-9436-4660-a3e4-5a542cfb10ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained models already exist. No need to download them.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./trained_models'):\n",
    "    # Download trained models (a zip file)\n",
    "    gdown.download(\"https://drive.google.com/uc?export=download&id=1dY3m_LdZvCLNOU1HnjjpVO9Fa8-qc2vk\", output='./trained_models.zip', quiet=False)\n",
    "    # Unzip the file\n",
    "    print('Extracting downloaded models...')\n",
    "    with zipfile.ZipFile('./trained_models.zip') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    os.remove('./trained_models.zip')\n",
    "    print(\"Completed!\")\n",
    "else:\n",
    "    print('Trained models already exist. No need to download them.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WYMK-mUPGe6"
   },
   "source": [
    "# Once you downloaded the models, you could configure to evaluate the trained models without re-training. Again we have 4 options, corresponding to 4 different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAPDCoCGOxKr"
   },
   "outputs": [],
   "source": [
    "# Option 1: Evaluate the basic model\n",
    "args.evaluate = True\n",
    "args.resume = './trained_models/ckpt.base.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxcaG4dFPO8v"
   },
   "outputs": [],
   "source": [
    "# Option 2: Evaluate the inverse weighting model\n",
    "args.evaluate = True\n",
    "args.resume = './trained_models/ckpt.inverse.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV45Fm6FPTsp"
   },
   "outputs": [],
   "source": [
    "# Option 3: Evaluate the sqrt_inverse weighting model\n",
    "args.evaluate = True\n",
    "args.resume = './trained_models/ckpt.sqrt_inv.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9onIhlctPf4K"
   },
   "outputs": [],
   "source": [
    "# Option 4: Evaluate the Label Distribution Smoothing (LDS) model\n",
    "args.evaluate = True\n",
    "args.resume = './trained_models/ckpt.lds.pth.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-zCfrN5av1A"
   },
   "source": [
    "# Train/Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSrzhog1gxyY",
    "outputId": "607f363a-f0f4-43da-a8dd-8dccc4a6aa22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Namespace(augment=True, batch_size=64, best_loss=100000.0, cpu_only=True, data_dir='./housing.data', dataset='bostonhousing', epoch=10, evaluate=True, gpu=None, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='fcnet1', momentum=0.9, optimizer='adam', print_freq=10, resume='./trained_models/ckpt.lds.pth.tar', reweight='none', schedule=[60, 80], start_epoch=0, store_name='bostonhousing_fcnet1_adam_l1_0.001_64', store_root='checkpoint', weight_decay=0.0001, workers=32)\n",
      "Store name: bostonhousing_fcnet1_adam_l1_0.001_64\n",
      "=====> Preparing data...\n",
      "Training data size: 358\n",
      "Validation data size: 66\n",
      "Test data size: 82\n",
      "=====> Building model...\n",
      "===> Checkpoint './trained_models/ckpt.lds.pth.tar' loaded (epoch [10]), testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/2]\tTime  0.424 ( 0.424)\tLoss (MSE) 32.239 (32.239)\tLoss (L1) 4.760 (4.760)\n",
      " * Overall: MSE 115.363\tL1 7.853\tG-Mean 4.842\n",
      " * Many: MSE 93.582\tL1 5.506\tG-Mean 2.859\n",
      " * Median: MSE 39.125\tL1 5.539\tG-Mean 4.301\n",
      " * Low: MSE 218.104\tL1 12.953\tG-Mean 10.386\n"
     ]
    }
   ],
   "source": [
    "# train.py Part 2: Train/Evaluate the model.\n",
    "args.store_name = ''\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    train_labels = np.loadtxt(args.data_dir+'.train')[:, -1]\n",
    "\n",
    "    train_dataset = BostonHousing(data_dir=args.data_dir+'.train', split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = BostonHousing(data_dir=args.data_dir+'.val', split='val')\n",
    "    test_dataset = BostonHousing(data_dir=args.data_dir+'.test', split='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    print(f\"Test data size: {len(test_dataset)}\")\n",
    "\n",
    "    # Random Seed\n",
    "    np.random.seed(999)\n",
    "    # random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    model = fcnet1()\n",
    "    if not args.cpu_only:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    # Loss and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    if not args.cpu_only:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "    # test with best checkpoint\n",
    "    print(\"=\" * 120)\n",
    "    print(\"Test best model on testset...\")\n",
    "    checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "    test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "    print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        if not args.cpu_only:\n",
    "            inputs, targets, weights = \\\n",
    "                inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        outputs = model(inputs, targets, epoch)\n",
    "\n",
    "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            if not args.cpu_only:\n",
    "                inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=10, low_shot_thr=2):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgVNKgFin9cx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deep_imbalance_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
